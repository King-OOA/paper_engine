\documentclass{article}

\usepackage[numbers]{natbib}
\citestyle{IEEEtran}

\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{color}
\newcommand{\SWITCH}[1]{\STATE \textbf{switch} (#1)}
\newcommand{\ENDPWITCH}{\STATE \textbf{end switch}}
\newcommand{\CASE}[1]{\STATE \textbf{case} #1\textbf{:} \begin{ALC@g}}
\newcommand{\ENDCASE}{\end{ALC@g}}
\newcommand{\CASELINE}[1]{\STATE \textbf{case} #1\textbf{:} }
\newcommand{\DEFAULT}{\STATE \textbf{default:} \begin{ALC@g}}
\newcommand{\ENDDEFAULT}{\end{ALC@g}}
\newcommand{\DEFAULTLINE}[1]{\STATE \textbf{default:} }
\newtheorem{mydef}{Definition}
\newtheorem{mylm}{Proposition}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% \title{An Efficient Adaptive Architecture for Muliti-pattern Matching}
% \author{Zhan Peng  Yuping Wang}
 %\maketitle

\begin{document}

\bibliographystyle{alpha}


\begin{center}
{\Large An Fast Engine for Large-scale String Pattern Matching}
\end{center}


\begin{abstract} 
  Multi-string matching is the core technique that is widely used in
  many applications. In general, it searches a text string for all
  occurrences of some string patterns. However, as the number of
  string patterns increases, most of the existing algorithms suffer
  from two issues: the long matching time, and the high memory
  consumption. To address these issues, in this paper, a fast matching
  engine is proposed for large-scale string matching problems. Our
  engine includes a filter module and a verification module. The filter
  module is based on several bitmaps, which is response for quickly
  filtering out the invalid positions in the text, while for each
  potential matched position, the verification module confirms true
  pattern occurrence. In particular, we design a compact data
  structure called Adaptive Matching Tree (AMT) as the verification
  module, in which each tree node saves some pattern fragments of the
  whole pattern set. The key point is that, the inner structure of
  each tree node is chosen adaptively according to the features of the
  corresponding pattern fragments, and this makes the whole AMT both
  time and space efficient for verifying. The experiments indicate
  that, our matching engine performs better than the prior algorithms,
  especially for large pattern sets.
\end{abstract}

{\textbf{Key words}: String Matching; Pattern Matching; Dictionary Matching}

due to the effectiveness of the filter and the compactness of the AMT
whether there are patterns starting at that position

\section{Introduction}
\label{sec:introduction}

String matching (SM) has been historically one of the key areas of
computer science. According to the number of strings to be matched
simultaneously, string matching technologies can be classified into
single string matching (SSM) and multi-string matching (MSM), and this
paper mainly focuses on the latter.

The MSM is a primitive but critical operation used in many
applications such as information retrieval, intrusion detection
system, information extraction, text editing, etc. In general, it
seeks all positions in a text string that match any of the string
patterns in a given pattern set. Efficient algorithms for MSM scan the
text only once, searching for all patterns
concurrently. Traditionally, these algorithms include two major
phases, i.e. the \textsf{preprocessing phase} and the \textsf{matching
  phase}. In the \textsf{preprocessing phase}, a suitable data
structure is constructed to store all the string patterns, and then
the \textsf{matching phase} scans and compares the given text with the
data structure for finding all occurrences every pattern in the
text. It is obvious that the time and space efficiency of the data
structure built in the preprocessing phase can have a significant
impact on the performance of the algorithm in the matching phase.

% Preferably, an algorithm scans the text once so that its performance
% should mainly depend on the size of the text rather than the features
% of pattern sets. However, the performance of lots of existing
% algorithms for MSM is affected seriously by the pattern sets. For
% instance, many algorithms, such as the classical \textsf{Wu-Manber}
% (\textsf{WM}) algorithm or the more recent \textsf{Pre-filter+AC}
% scheme, can not efficiently deal with the pattern sets containing very
% short patterns. When encountering such pattern sets, the performance
% of these algorithm degrades dramatically. This is due to the internal
% design of the data structure which is used for
% matching. {\color{red}{Therefore, it's quite valuable to design a data
% structure that can efficiently support pattern sets with various
% features. Using this kind of data structure can improve the robustness
% of MSM.}}
 
In the big data context nowadays the number of patterns to be matched
becomes huge in some applications, for instance, the database of a
modern anti-virus/intrusion-detection system can easily contain
millions, or even tens of millions number of virus
signatures/rules. However, the existing algorithms are not suitable
for such applications with very large pattern sets. Because on the one
hand, the throughout of these methods can not keep up with the demand
of the applications as the number of patterns increases, which is due
to the poor scalability of the data structures used by these methods,
and on the other hand, the data structure of several algorithms (such
as the \textsf{AC} algorithm) is not space efficient, which means they
can not process large pattern sets with limited memory. Therefore it
is very valuable to design space-efficient data structures with good
scalability to support applications requiring large-scale string
matching.

In this paper, we propose a multi-string matching engine for large
string pattern sets. Similarly to the traditional approaches, our
strategy consists of the preprocessing phase and the matching
phase. In the preprocessing phase, the matching engine is constructed
based on the pattern set. Then in the matching phase, the engine takes
the text string as input, and searches the occurrences of each
pattern in the text.

The proposed engine consists of a filter module and a verification
module. The function of the filter module is to find the potential matched
positions in the text. Once such a position is found, the verification
module confirms whether there are patterns starting at that
position. The filter we adopted is proposed by \cite{Lee2013}, which
is based on several bitmaps and a group of simple bitwised \emph{and}
and \emph{shift} operations. In the matching phase, the engine queries
the bitmaps to check the validity of the current position: if the
current position is a potential matched position, the verification
unit is invoked; otherwise, the engine skips over this position (as
well as the next few positions if possible). In particular, the
authors proved that the proposed filter is optimal in the sense that
it is able to utilizing all previous query results, which means the
filter can skip as many invalid positions as possible. For the
verification module, we design a compact tree-like data structure called
Adaptive Matching Tree (\textsf{AMT}) based on the pattern
set. Specifically, the whole pattern set is gradually partitioned into
several small parts first, and then the pattern fragments of each
small part is stored in a separate node of the AMT for subsequent
matching. The key point is that, the inner structure of each tree node
is selected adaptively according to the features of the pattern
fragments, and this adaptivity guarantees a high matching speed as
well as a low memory consumption for each tree node, as a result, the
whole AMT would be both time and space efficient for verifying in the
matching phase.

The rest of this paper is organized as follows: Section
\ref{sec:related works} reviews some related works in MSM. Section
\ref{sec:notations} lists the notations and terminology used in this
paper. Section \ref{sec:ADSM} describes our matching engine in
details. Section \ref{sec:experiments} presents the performance
evaluations. Section \ref{sec:conclusion} concludes the paper and
proposes some open problems.

\section{Related Works}
\label{sec:related works}

Because of the importance of PM, several efficient algorithms have
been proposed in the past decades. Well-known algorithms for PM
include Knuth-Morris-Pratt (\textsf{KMP}) \cite{Knuth1977},
Boyer-Moore (\textsf{BM}) \cite{Boyer1977}, Wu-Manber (\textsf{WM})
\cite{Wu1994}, and Aho-Corasick (\textsf{AC}) \cite{Aho1975}. The
\textsf{KMP} and \textsf{BM} algorithms are appropriate for SPM, but
not suitable for MPM. The \textsf{AC} algorithm is a generalization of
the \textsf{KMP} algorithm, by preprocessing the patterns and building
a deterministic finite automaton (DFA) that can match multiple
patterns simultaneously. It is the first algorithm for MPM that has a
linear time complexity. However, the \textsf{AC} algorithm is
alphabet-sensitive and requires a huge amount of memory to build the
DFA, which limits its application in large-scale pattern matching.

To reduce the memory requirement of \textsf{AC}, Tuck[2004] proposed a
algorithm called \textsf{Bitmapped AC}. This algorithm uses the bitmap
data structure to replace the conventional child pointers in each
state. In addition, it uses a path compression technique which can
merge successive single-child states into one state. It is reported
that, the compression optimization of \textsf{Bitmapped AC} results in
a 50 times reduction in memory consumption over the default
\textsf{AC} algorithm. However, since traversal from node to node
requires checking a bit in a bitmap and then performing a sum up to
256 prior bits in the bitmap, the average performance of this
algorithm is not high.  Another algorithm to reduce the space
requirement is the \textsf{Compressed AC} algorithm proposed by
Bremeler-Barr[2011]. In addition to use a similar path compression
method, this algorithm also develops a technique that can eliminate
the leaf states. The experiment shows that \textsf{Compressed AC} can
reduce at most $60\%$ memory requirement compared with the
traditionally \textsf{AC} algorithm. But the similar problem is, it
only works well for a few pattern sets.  Lee[2013] proposed an
algorithm called \textsf{Pre-filter+AC}. As the name shows, this
algorithm consists of a pre-filter and a verification engine. The
function of the pre-filter is to query data structures built from
patterns to find the starting positions of potential pattern
occurrences. Once a suspicious starting position is found, the
verification engine confirms true pattern occurrence. The pre-filter
uses a bit vector, called master bitmap, with simple bitwise-AND and
shift operations to accumulate query results. While the verification
engine, which is a modification of the AC automaton, checks all
candidate patterns simultaneously rather than sequentially. However,
as the author stated, the pre-filter is suitable for patterns of
moderate or large lengths. For short patterns, its throughput
degrades.

The \textsf{WM} algorithm is another efficient algorithm for MPM which
is known to be faster than \textsf{AC} in a typical matching. The
\textsf{WM} algorithm is an adaptation of the \textsf{BM} algorithm to
multiple patterns. It uses a similar ``shift idea'' of the \textsf{BM}
algorithm, but the shift of \textsf{WM} is not based on a single
character as the \textsf{BM} does, instead, it is based on a
$B$-$gram$ that is a character block consists of $B$ characters ($B$
is usually set to 2 or 3 in practice). In a typical search, its time
complexity is $O(B \cdot n/lsp)$, where $lsp$ is the length of the
shortest pattern in the pattern set. Therefore, the performance of WM
is seriously affected by $lsp$.

To improve the \textsf{WM} algorithm, Zhou \cite{Zhou2007} proposed an
algorithm called \textsf{MDH}. Instead of using the first $m$
characters as the signature of each pattern, \textsf{MDH} adopts a
heuristic strategy to select the optimum $m$ consecutive characters as
the signature of each pattern. Similarly, Zhan \cite{Zhan2014}
proposed a different way to search for the optimal signatures of the
patterns, additionally, this algorithm designs an index structure to
reduce the time for searching the candidate patterns in the Hash
Table. These algorithms can improve the performance of \textsf{WM} for
common patterns but they do not solve the inefficiency caused by short
patterns. To address this problem, Zhang \cite{Zhang2009} proposed the
High Concurrence WM algorithm (\textsf{HCWM}), which separates the
patterns whose length is less than 4 from the pattern set. For these
short patterns, the algorithm establishes independent data structures
and uses different matching routines. This method can reduce the
effect of short patterns, since short patterns are matched
independently and concurrently. Another algorithm for this problem is
the \textsf{$L^{+1}$-MWM} proposed by Choi \cite{Choi2011}, which
minimizes the performance degradation caused by the short patterns by
appending characters to these patterns. It is reported that, the
\textsf{$L^{+1}$-MWM} improves the performance of WM by as mush as
$20\%$ in average, moreover, when $lsp < 5$ the \textsf{$L^{+1}$-MWM}
gains an $38.87\%$ enhancement.

For large scale pattern matching, which becomes more and more
important in big data context, Le \cite{Le2013} presented a
Memory-efficient Architecture for large-scale String Matching
(\textsf{MASM}) based on a pipelined binary search tree. It uses a
technique called ``leaf-attaching'' to compress the given pattern
set. The compressed pattern set are then transformed to a pipelined
binary tree tree which will be used in the matching phase. It is
reported that, the \textsf{MASM} achieves a memory efficiency (defined
as the ratio of the amount of the required storage in bytes and the
size of the pattern set in number of characters) of 0.56 for the
Rogets dictionary and 1.32 for the Snort rule set. Moraru
\cite{Moraru2012} also presented a memory-efficient and
cache-optimized algorithm for large pattern sets. This algorithm
builds upon the Rabin-Karp \cite{Karp1987} SPM algorithm and
incorporates a new \textsf{feed-forward} bloom filter which takes into
account the memory hierarchy of modern computers. This algorithm is
also well suited for implementation on GPUs which enables the matching
to be done in parallel.

There are also studies that exploited the capacity of hardware to
accelerate the proposed MPM engines. Agarwal \cite{Agarwal2013}
proposed a hardware architecture which enables high throughout for
Information Extraction applications. Instead of using the common DFA
based approach, this architecture employs a novel hashing based
scheme. The DFA based approaches (such as \textsf{AC}) typically
process one character every cycle, while the proposed hash based
scheme can process a string token of several characters every cycle,
thus achieves higher throughout than the DFA based approaches. Zhang
\cite{Zhang2015} proposed an GPU-based parallel algorithm
\textsf{G-PEBF} which uses the Extended Bloom Filter (EBF). It divides
the pattern set into $N$ subsets where the lengths of the patterns in
each subsets are the same. Then it constructs an EBF for each subset
and uses $N$ threads to simultaneously process the subsets in parallel
on the GPU. The performance of \textsf{G-PEFB} highly relies on the
number of threads used during matching, and it is also not applicable
for pattern sets containing very short patterns.

Nowadays, some variants of the classical MPM problem have been
proposed. Tomohiro \cite{I2015} introduced a variant of MPM, where the
pattern set is given in a compressed form that can be represented by a
straight line program (SLP). For a given SLP-compressed pattern set of
size $n$ and height $h$, which represents $m$ patterns of total length
$N$, they present an $O(n^2log^N)$-size variant of AC automaton that
recognizes all occurrences of the patterns in $O(h + m)$ running time
per character. Khancome \cite{Khancome2013} proposed an algorithm for
the dynamic MPM problem. This algorithm uses the \textsf{inverted
  lists} data structure to allow the pattern set to be updated
dynamically in a optimal time. Amir \cite{Amir2015} introduced an
algorithm for the gapped MPM problem, where each pattern in the
pattern set is a sequence of sub-patterns separated by bounded
sequences of do not cares. Neuburger \cite{Neuburger2012} presented
the first efficient algorithm that operates in small space for the
2-dimensional MPM problem.

\section{Notations}
\label{sec:notations}

Let $\Sigma$ be an \textsf{alphabet} consisting of a finite number of
character symbols. (In this paper we will only focus on the case that
$\Sigma$ is the ASCII character set, i.e. $|\Sigma| = 256$ and each
character takes one byte memory.) Given the alphabet $\Sigma$, a
\textsf{string} as well as its \textsf{substrings} over
$\Sigma$ can be defined as follows:\\
\\
\textbf{Definition 1.} A \textsf{string} over $\Sigma$ is a sequence
consisting of a finite number of characters from $\Sigma$. A
length-$n$ string is represented by $S = c_1c_2..c_n$, where the
$i$-th character of $S$ is: $S[i] = c_i \in \Sigma$
$(1 \leq i \leq n)$. The length of $S$ is denoted by $|S|$. A
\textsf{substring} of $S$ is a sequence consisting of any consecutive
characters of $S$. The substring of $S$, which starts at
position $i$ and has length $len$, is denoted by $S[i,\,len]$.\\
\\
Given a string $S$, its \textsf{prefix} and \textsf{suffix},
which are both special substrings of $S$, are defined as follows:\\
\\
\textbf{Definition 2.} The length-$m$ \textsf{prefix} of a string $S$,
which is denoted by $S^{+m}$, is the substring that consists of the
first $m$ characters of $S$, i.e. $S^{+m}=S[1,\,m]$.  While the
\textsf{suffix} of $S$ that starts at position $m+1$, is the substring
left after removing the length-$m$ prefix of $S$. It is
denoted by $S^{-m}$, i.e. $S^{-m} = S[m+1,\,|S|-m]$.\\
\\
Actually, a string $S$ can be regarded as a prefix/suffix of itself,
i.e. $S=S^{+|S|}=S^{-0}$. Finally, the MSM problem can be
formally defined as follows: \\
\\
\textbf{Definition 3.} Given a text string $T$ and a pattern set
$P=\{p_1,\,p_2,\,\dots,\,p_k\}$, where $T$ and $p_i$
$(1 \leq i \leq k)$ are all strings over $\Sigma$.  The multi-string
pattern matching is to find all occurrences of every pattern of $P$ in
$T$, more formally, to build the result set
$R = \{(i,\, p_j)\;|\; p_j \in P\; and\,\; T[i,\,|p_j|]=p_j\}$.


\section{The Filter Module}
\label{sec:filter}

As mentioned before, our engine consists of a filter module and a
verification module. In this section, we introduce the former, while
in the next section the latter will be illustrated.

% the whole pattern set is transformed to a compact tree-like structure
% called Adaptive Matching Tree (AMT), where each tree node is built
% adaptively to save the information of a small part of the pattern
% set. While in the matching phase, the built AMT is compared against
% the text string to search patterns in that string. The framework of
% constructing the AMT from the pattern set is introduced in Section
% \ref{subsec:amt}, and using the AMT to search patterns in the text
% string is described in Section \ref{subsec:matching}, finally
% adaptively creating the tree nodes is presented in Section
% \ref{subsec:nodes}.

The filter we adopted in our engine is proposed by \cite{Lee2013},
which is an improvement of the filter in the WM algorithm. Given the
pattern set, as in the WM algorithm, only the length-$lsp$ prefix of
each pattern are considering in constructing the filter, where $lsp$
is the \emph{length of the shortest pattern} in the pattern
set. Similar to WM, the filtering is based on character blocks (a
character block is a merely short string consisting of only a few
characters) rather than single characters. Given a block size $k$, the
filter includes $lsp-k+1$ membership query bitmaps denoted as $B_1$,
$B_2$, \dots, and $B_{lsp-k+1}$, where each bitmap is a vector of $N$
bits. Let $B_j[i]$, $0 \leq i \leq N - 1$, $1 \leq j \leq lsp-k+1$, be
the $i$-th bit of the $j$-th query bitmap, and $B_j[i]$ is set to 1
iff there exists a pattern $p_m$ such that $hash(p_m[j,k]) = i$
($p_m[j,k]$ is a $k$-symbol block starting at position $j$ of $p_m$),
where $hash$ is a hash function that maps a $k$-symbol block to an
integer in the range of $[0, N-1]$.

Fig shows an example, assume that alphabet
$\Sigma = \{0, 1, 2, \dots, 9\}$ and pattern set
$P = \{p_1,\; p_2,\; p_3\}$, where $p_1 = 04648753$, $p_2 = 30692$,
and $p_3 = 614621$. Obviously, there is $lsp = |p_2|= 5$. Assume that
$k = 2$, $N = 100$, and the hash function is the identity mapping. For
such a setting, there are $lsp - k + 1 = 4$ query bitmaps, i.e. $B_1$,
$B_2$, $B_3$, $B_4$, and in particular, we have $B_1[i] = 1$ iff $i$ =
4, 30, or 61; $B_2[i] = 1$ iff $i$ = 6, 14, or 46; $B_3[i] = 1$ iff
$i$ = 46, 64, or 69; and $B_4[i] = 1$ iff $i$ = 48, 62, or 92.

% In addition, another bitmap of $lsp-k+1$ bits, which is called
% \emph{master bitmap} and denoted by
% $MB = mb_1mb_2 \dots mb_{lsp-k+1}$, is adopted to accumulate the
% previous query results and act as the current state of the filter.

Once the query bitmaps have been constructed, they can be used to
filter the positions of the input text string. During filtering, a
sliding window $W$ of length $lsp$ is adopted to select a segment of
the text string $T$. Initially, $W$ is aligned with the beginning of
$T$, so that the substring of $T$ contained in $W$ is $T[1,lsp]$. In
general, assume that $W$ is moved to the $i$-th position of $T$, and
the substring of $T$ contained in $W$ is $T[i,lsp]$. Then the last
$k$-symbol block of $T[i,lsp]$, which is $T[i+lsp-k, k]$, is used to
query all the bitmaps. Let single bit $qb_j$ be the report of $B_j$
($1 \leq j \leq lsp - k + 1$), and $qb_j=1$ iff
$B_j[hash(T[i+lsp-k,k])] = 1$. Let bitmap
$QB = qb_1qb_2 \dots qb_{lsp-k+1}$ to denote the current query result
and another \emph{master bitmap} $MB = mb_1mb_2 \dots mb_{lsp-k+1}$ to
accumulate the previous query results and act as the current state of
the filter. Initially, all the bits of $MB$ are set to 1. After
fetching a query result $QB$, the master bitmap is updated by
$MB = MB \; \& \; QB$, where \& is the bitwised-AND operation. For the
updated $MB$, if $mb_{lsp-k+1} = 1$, position $i$ is considered as a
potential matched position and the verification module is
invoked. Otherwise, the sliding window $W$ is advanced by $lsp-k+1$
positions if all the bits of $MB$ are 0's, or $lsp-k+1-r$ positions if
$mb_r=1$ and $mb_i=0$, for $r < i \leq lsp-k$. The positions skipped
by $W$ are filtered out. If $W$ is determined to be advanced by $h$
positions, $MB$ is right-shifted by $h$ bits and filled with 1's for
the holes left by the shift. Particularly, it is proved in
\cite{Lee2013} that the proposed filter using master bitmap and simple
bitwised AND and SHIFT operations is optimal in the sense that it is
able to utilize all previous query results, which means the filter can
filter out as many positions as possible in the text.

As an example, consider the query bitmaps constructed above and assume
that $T=23764614621$. Initially, the matching window $W$ contains
23764 and $MB = 1111$. The last 2-symbol block of $W$, i.e. 64, is
used for the first query, and the query result is $QB=0010$. Since
$MB\; \&\; QB = 0010$, $W$ is advanced by 1 position (which means the
first position of $T$ is filtered out), and then $MB$ is updated as
$1001$ by right shift. For the second position, the substring
contained in $W$ is 37646.  This time, block 46 is used to query the
bitmaps and the query result is $QB=0110$. Since $MB\; \& \; QB=0000$,
$W$ moves forward by 4 positions to position 6 (which means positions
$2 \sim 5$ are filled out), and $MB$ is updated as 1111. For position
6, $W$ contains 61462 and block 62 is used for querying and fetches
$QB = 0001$. Since $MB\; \& \; QB = 0001$, position 6 is a potential
matched position, and then the verification module is invoked. After
checking, the pattern $p_3=614621$ is detected at position 6. Then,
$W$ is advanced by 4 positions to position 10. However, since the
length of the remaining input text in $W$, i.e. 21, is smaller than
$lsp$, which means there can not be any pattern starting at that
position, the whole scanning procedure is terminated.

% The core idea of the filter is to use the length-$lsp$ prefix of each
% pattern as the \textsf{signatures}, and then use the signatures to
% check the validity of each position. In particular, if the current
% position of the text (roughly) matches a signature, which means the
% position may match the corresponding pattern, the current position is
% a potential matched position; otherwise, if the current position does
% not match any signature, this position can not match any pattern
% either and can be safely filtered out.

\section{The verification module}
\label{sec:verification}

The verification module is the core of the whole matching engine,
since it determines the worst case performance of the engine. The
verification module in our engine is based on a new data structure
called \emph{Adaptive Matching Tree} (AMT) which is a modification of
the classical \emph{trie} structure. Like the \emph{trie}, but in a
more flexible and space efficient way, the AMT enables a fast
membership query which decides whether a given target string exists in
the pattern set. In the next subsections, we will introduce the
construction of the AMT, constructed AMT, and a some further
optimization techniques.

\subsection{AMT Construction}
\label{subsec:amt}

The AMT is constructed from the pattern set, and in general, this
construction includes five major steps:

\begin{enumerate}
\item Create a suffix set $SF$, and put each pattern of the pattern
  set into $SF$, where each pattern is regarded as a suffix of itself.
\item Compute the \emph{length of the shortest suffix} ($lss$) in
  $SF$. For each suffix in $SF$, remove its length-$lss$ prefix, and
  all the removed prefixes form a prefix set $PF$. Discard the
  repetitions in $PF$, and calculate $|PF|$ which refers to the
  \emph{number of distinct prefixes} ($ndp$).
\item Create a tree \emph{node} to hold the prefixes in $PF$. The
  \emph{node} is actually an index, in which each prefix serves as a
  key. The inner structure of the \emph{node} is constructed
  adaptively according to $ndp$ and $lss$ that have been computed in
  step 2.
\item Divide $SF$ by putting together all the suffixes with same
  prefix removed, to form a sub-suffix-set. Associate each
  sub-suffix-set with the corresponding prefix in the tree
  \emph{node}.
\item For each newly created sub-suffix-set in step 3, repeat the same
  procedure from step 2.
\end{enumerate}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{./eps/AMT}
  \caption{Constructing the AMT from a pattern set. Each arrow
    in the AMT indicates a child pointer.}
  \label{fig:AMT}
\end{figure}

As an example, Figure \ref{fig:AMT} shows of the construction of AMT
from a pattern set $P = \{p_1,\, p_2,\, \dots,\, p_{13}\}$. Note that,
since each pattern is a suffix of itself, the whole pattern set $P$
can be also regarded as a suffix set $SF$. First, since $p_6$ is the
shortest pattern(suffix) in $P$, we have $lss = |p_6| = 2$. Then the
length-2 prefix of each pattern(suffix) is removed, and the removed
prefixes form a prefix set:
$PF = \{p_1^{+2} = aa,\; p_2^{+2} = aa,\; \dots,\; p_{13}^{+2} =
cc\}$. After discarding the repetitions in $PF$, only 3 distinct
prefixes are left: $PF = \{aa,\, bb,\, cc\}$, and we have
$ndp = |PF| = 3$.

Then the first tree node, which is actually the root of AMT, is
created to hold the prefixes in $PF$. Each prefix serves as a
\emph{key} in the root and has a pointer (initialized to $NULL$) which
will be used to associate with the child node in the future. As will
be seen latter, the structure of root is selected adaptively according
to two parameters: $lss=2$ and $ndp=3$.

After creation of the root, the suffixes left in $P$ are grouped based
on their lost 2-symbol prefixes: those suffixes having the same
prefixes removed are grouped together to form a sub-suffix-set. In
order to record the correspondence between the sub-suffix-set and the
lost prefix, the tuple (\emph{node.key,\; sub-suffix-set}) is used:
the suffixes in the \emph{sub-suffix-set} have the same prefix
\emph{key} removed, where the \emph{key} is held as a key in the newly
created \emph{node}. Using this notation, there are three tuples
created after grouping the suffixes left in $P$:
$tp_1 = (root.aa,\; SF_1=\{p_1^{-2},\, p_2^{-2},\, p_3^{-2},\,
p_4^{-2},\, p_5^{-2},\, p_7^{-2}\})$,\,
$tp_2 = (root.bb,\; SF_2=\{p_8^{-2},\, p_9^{-2},\, p_{10}^{-2}\})$ and
$tp_3 = (root.cc,\; SF_3=\{p_{11}^{-2},\, p_{12}^{-2},\,
p_{13}^{-2}\})$. This indicates that $P$ is divided into three
sub-suffix-sets: $SF_1$, $SF_2$, $SF_3$, and each sub-suffix-set
corresponds to a prefix held as a key in the root (it also means that
the root may have three child nodes). Note that, the shortest
suffix(pattern) $p_6$ has disappeared in the sub-suffix-sets, since
after removing the length-2 prefix of $p_6$, there is nothing left.

The same procedure is repeated upon each of the three created
sub-suffix-sets. As a further example, $SF_1$ of $tp_1$ is
processed. First, since $lss = |p_7^{-2}| = 6$, the length-$6$ prefix
of each suffix in $SF_1$ is removed to form a prefix set:
$PF_1 = \{(p_1^{-2})^{+6},\, (p_2^{-2})^{+6},\, (p_3^{-2})^{+6},\,
(p_4^{-2})^{+6},\, (p_5^{-2})^{+6},\, (p_7^{-2})^{+6}\}$. After
discarding the duplicates in $PF_1$, only three distinct prefixes are
left: $PF_1 = \{a^6,\, e^6,\, b^6\}$ (we use the notation $c^n$ to
denote a length-$n$ string consisting of the same character $c$). Then
a new tree node $node_1$ is built adaptively upon $PF_1$ to hold its
prefixes as keys.  According to the first component of $tp_1$
(i.e. $root.aa$), $node_1$ is associated with the key $aa$ in the root
by a child pointer, which makes it to be the first child of root. Once
again, the suffixes left in $SF_1$ are grouped based on their lost
length-6 prefixes, which formed three new tuples:
$tp_4 = (node_1.a^6,\; SF_4=\{p_1^{-8}\})$,
$tp_5 = (node_1.e^6,\; SF_5=\{p_2^{-8},\, p_3^{-8},\, p_4^{-8}\})$ and
$tp_6 = (node_1.b^6,\; SF_6=\{p_5^{-8}\})$.

Note that, in order to mark the end of a pattern, once the last part
of the pattern has been removed and then stored in a node, that last
part is marked with an asterisk(*) in the corresponding node. As a
result of this, any path from the root to a key that marked with an
asterisk represents a complete pattern. From this it can be seen that,
the patterns are stored implicitly in the AMT and can be reconstructed
from the paths ended with an asterisk.

During the construction of AMT, we use a \emph{breadth-first} strategy
to process the sub-suffix-sets and create tree nodes (which means the
next sub-suffix-set to be processed is $SF_2$ rather than $SF_4$). For
the purpose of tracing the order in which the sub-suffix-sets are
processed, a \emph{first-in-first-out} queue is employed to maintain
the created tuples. The tuple at the beginning of the queue contains
the sub-suffix-set that will be processed next, while the newly
created tuples are inserted to the end of the queue in order. In our
example, the root node is built initially, then $tp_1$, $tp_2$ and
$tp_3$ are inserted to the queue. Next $SF_1$ of $tp_1$ is processed
and the newly created tuples $tp_4$, $tp_5$ and $tp_6$ are inserted to
the queue in order. Subsequently, $SF_2$, $SF_3$, $SF_4$, $SF_5$,
$SF_6$, $\dots$ are processed in sequence. Once there is no tuple left
in the queue, the whole AMT has been constructed.

\begin{algorithm}
  \caption{Constructing the AMT}\scriptsize
  \label{alg:amt}
  \begin{algorithmic}[1]
    \REQUIRE The pattern set $P$
    \ENSURE The corresponding AMT
    \STATE
    \STATE $Q \leftarrow$ Create an empty queue
    \STATE $push\_queue((NULL,\,P),\; Q)$
    \STATE
    \WHILE{$Q$ is not empty}
    \STATE $(parent\_node.key,\; SF) \leftarrow pop\_queue(Q)$
    \STATE $lss \leftarrow$ The length of the shortest pattern in $SF$
    \FOR{each $suf \in SF$}
    \IF{$|suf|=lss$}
    \STATE Mark $suf^{+lss}$ to be the pattern end
    \ENDIF
    \STATE Remove $suf^{+lss}$ of $suf$, and put $suf^{+lss}$ into $PF$
    \ENDFOR
    \STATE Discard the repetitions in $PF$, and let $ndp \leftarrow |PF|$
    \STATE According to $lss$ and $ndp$, adaptively create a
    $new\_node$ to hold the prefixes in
    $PF$
    \IF{$(parent\_node.key = NULL)$}
    \STATE $root \leftarrow new\_node$
    \ELSE
    \STATE Associate $new\_node$
    with $parent\_node.key$ by a child pointer
    \ENDIF
    \STATE $TP \leftarrow \{(new\_node.pf,\, SSF) \mid SSF \subseteq SF\; and
    \ \forall \ p,\,q \in SSF: p,\,q$ have the same length-$lss$ prefix
    $pf$ removed\}\
    \FOR{each $tp \in TP$}
    \STATE $push\_queue(Q,\,tp)$
    \ENDFOR
    \ENDWHILE
    \STATE
    \RETURN $root$.
  \end{algorithmic}
\end{algorithm}

The framework of constructing the AMT from a pattern set is
illustrated in Algorithm \ref{alg:amt}. Firstly, an empty queue $Q$ is
created in line 2 for maintaining the tuples. Then the tuple
$(NULL, P)$ is inserted to $Q$ in line 3 by function $push\_queue$
which always inserts an element to the end of the queue. Since $P$ is
the initial pattern set here, the first component of the tuple is
$NULL$, which means there is no prefix removed from $P$ yet. By this
way, the creation of the root can be combined with the creation of
other tree nodes in the following \textbf{while} loop.

If $Q$ is not empty, the \textbf{while} loop body from line 5 to line
25 constructs a tree node based on the first tuple in $Q$. The tuple
at the beginning of $Q$ is fetched by the $pop\_queue(Q)$ function in
line 6: the suffix set to be processed is assigned to $SF$, and the
corresponding prefix of $SF$ in the parent node is denoted by
$parent\_node.key$. Then the $lss$ of $SF$ is computed in line 7. In
the inner \textbf{for} loop from line 8 to line 13, the length-$lss$
prefix of each suffix in $SF$ is removed, and the prefixes are
collected to form the prefix set $PF$. If a prefix is the last part of
some pattern, it is marked as a pattern end. Line 14 discards the
repetitions in $PF$, after that, a new tree node is created adaptively
to hold the prefixes $PF$ in line 15. The \textbf{if} statement in
line 16 decides whether the newly created node is the root node or
not: if the prefix component of the current tuple is $NULL$, the new
node is the root node as stated before; otherwise, the new node is a
child node which will be then associated with the corresponding prefix
in its parent node. Next in line 21, the suffixes left in $SF$ are
grouped based on their lost prefixes, and each group and the
corresponding prefix forms a tuple. Finally, the created tuples are
inserted orderly to the end of $Q$ by the $push\_queue$ function. Once
there is no tuple left in $Q$, the whole AMT has been completely
constructed, and the root of AMT is returned.

\subsection{Verification}
\label{subsec:matching}

As stated in the filter module, once a suspicious position in the text
has been detected, the verification module checks that position for
real pattern occurrences. Specifically, for such a suspicious position
$i$, the engine searches a sequence of substrings of the text, where
the first substring starts at $i$, in the corresponding nodes of AMT
from the root. If any substring successfully matched a key that is
marked as a pattern end in some node, a pattern is declared to be
found at position $i$. However, if there is a mismatch or the
searching goes beyond the leaves of AMT, the engine immediately
forwards to the next position $i+1$ and invokes the filter again.

The pseudocode of the verification procedure is shown in Algorithm
\ref{alg:matching}. Suppose $i$ is a suspicious position of the text
detected by the filter module. The variable $m\_len$ is the total
length of the sub-strings of the text who have matched with keys in
AMT nodes. The variable $node$ points to the current searching node in
the AMT. In addition, since all the keys in a node have the same
length, the notation $key\_len(node)$ is used to denote the length of
the keys in $node$.

\begin{algorithm}
  \caption{Verification}\scriptsize
  \label{alg:matching}
  \begin{algorithmic}[1]
    \REQUIRE ~~\\
    A suspicious position $i$ in the text $T$\\
    The AMT built from the pattern set $P$\\
    \ENSURE ~~\\
    The patterns (if any) start at $i$ 
    \STATE
    \STATE $m\_len \leftarrow 0$
    \STATE $node \leftarrow root$ of AMT
    \STATE
    \WHILE{$node \neq NULL$ and $\exists key \in node: key =
      T[i+m\_len, \, key\_len(node)]$}
    \STATE $m\_len \leftarrow m\_len + key\_len(node)$
    \IF{$key$ is marked as a pattern end}
    \STATE Report that the pattern $T[i,\,m\_len]$ is found at position $i$
    \ENDIF
    \STATE $node \leftarrow$ The child of $node.key$
    \ENDWHILE
  \end{algorithmic}
\end{algorithm}

For the suspicious position $i$, the \textbf{while} loop from line 5
to line 11 checks whether there are patterns starting at $i$. The
checking starts form the root of AMT: if the corresponding sub-string
$T[i+m\_len, \, key\_len(node)]$ matches some $key$ in the current
$node$, the totally matched length $m\_len$ is increased by the length
of the key. At the same time, if the matched $key$ is also marked as a
pattern end, the verification module will report that a pattern,
i.e. $T[i,\,m\_len]$, is found at position $i$. Then the checking
transfers to the child of $node.key$ and makes that child node to be
the current node. Once there is a mismatch or the current node goes
beyond the leaves of AMT (i.e.  $node = NULL$), the checking
terminates, and the engine forwards to the next position $i+1$.

Note that, since the nodes in AMT have various types of inner
structures, the search of the target string in a node, must use the
search routine specified to the type of that node.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{./eps/match}
  \caption{The verification procedure for the position $i$ in the
    text.}
  \label{fig:matching}
\end{figure}

Next, an example is given in figure \ref{fig:matching} to illuminate
the verification for a suspicious position $i$ of a given text string
$T$, using the AMT built in section \ref{subsec:amt}. The verification
starts from the root of AMT: according to $key\_len(root)=2$, the same
length sub-string starting at position $i$ of $T$, which is
$T[i,\,2]=aa$, is taken to check whether it's one of the keys in
root. The result is: $aa \in root$, and $aa$ is also marked as a
pattern end. Therefore the pattern $T[i,\,2]=aa$ ($p_6$ in $P$) is
found at position $i$. Then the checking transfers to $node_1$, which
is the child node of $root.aa$.

At $node_1$, since $key\_len(node_1)=6$, the same length sub-string
$T[i+2,\,6]=e^6$, which follows $T[i,\,2]$, is trying to match with
some key in $node_1$. The result is that: $e^6 \in node_1$ but it is
not a pattern end. Then the checking simply transfers to $node_5$,
which is the child of $node_1.e^6$.

At $node_5$, since the next sub-string $T[i+8,\,5]=u^5 \in node_5$ and
$u^5$ is marked as a pattern end, another pattern
$T[i,\,13]=a^2e^6u^5$ ($p_4$ in $P$) is found at position $i$. Then
the checking goes to the child of $node_5.u^5$, i.e.  $node_{10}$.

At $node_{10}$, since $T[i+13,\,4]=sscc \notin node_{10}$, the
verification for position $i$ terminates. And next, the engine will
forward to position $i+1$ and invoke the filter module for that
position.

From above it can be seen that, each verification procedure actually
corresponds to a \emph{verification path} in the AMT, whose tree nodes
have been compared with the text in the former process. In particular,
the verification path of the given example is:
$root \rightarrow node_1 \rightarrow node_5 \rightarrow node_{10}$,
witch is indicated by the dashed arrows in Figure \ref{fig:matching}.


\subsection{Adaptive creation of the tree nodes}
\label{subsec:nodes}

As stated in section \ref{subsec:amt}, each node of AMT has a specific
index structure to hold the prefixes of a prefix set as the keys. The
type of the index structure is chosen adaptively according to the
features of the prefix set. Since all the keys in the prefix set have
the same length, the features of the prefix set can mainly be
characterized by two values: the \emph{length} and \emph{number} of
the keys in the prefix set. These two values are respectively
symbolized by $lss$ and $ndp$ in Section \ref{subsec:amt}.

In this work, three different kinds of structures,
i.e. \emph{character map}, \emph{string array} and \emph{hash table}
can be adopted for prefix sets with different $lss$ and $ndp$
combinations. Next, the detailed descriptions of these structures will
be presented.

\subsubsection{Character map}

Given a prefix set, if the length of keys is equal to 1 ($lss=1$),
i.e. each key in the prefix set is just a single character, the
space-efficient \emph{character map} structure, which was proposed by
Leis \cite{Leis2013}, will be adopted as the tree node. There are four
sub-types of character maps with different capacities for different
number of keys ($ndp$), where $1 \leq ndp \leq 256$.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{./eps/Maps}
  \caption{The four sub-types of character maps. The triangles
    represent the corresponding child nodes of the keys.}
  \label{fig:character map}
\end{figure}

Figure \ref{fig:character map} illustrates the four sub-types of
character maps which are named according to their maximum
capacity. Instead of using one array of (\emph{key, child pointer})
pairs, the key part and the child pointer part are stored in the
separate arrays, which is able to keep the node structure compact
while supporting efficient search.

\begin{itemize}
\item \textbf{Map 4} (for $1 \leq ndp \leq 4$): The smallest map type
  can store up to 4 keys. It uses an array of 4 entries for keys and
  another array of the same length for child pointers. The keys and
  child pointers are stored at corresponding positions in their
  arrays, and the keys are sorted according to their ACSII
  values. Once the target character is found in the key array, its
  child pointer can be located through the same position in the
  pointer array. Figure \ref{fig:character map} (a) shows a Map 4
  structure with three keys: $f$, $n$ and $p$, where the triangles
  with the keys inside represent the corresponding child nodes.

\item \textbf{Map 16} (for $5 \leq ndp \leq 16$): This map type is
  used for storing 5 to 16 keys. It has the similar structure as Map
  4, but both arrays have up to 16 entries. A target character can be
  retrieved efficiently by a binary search in the key array. Figure
  \ref{fig:character map} (b) shows a Map 16 structure with 14 keys.
  

\item \textbf{Map 48} (for $17 \leq ndp \leq 48$): As the number of
  keys increases, searching in the key array becomes
  expensive. Therefore, maps with more than 16 (but less than 49) keys
  do not store the keys explicitly. Instead, a 256-element \emph{index
    array} is used, which can be directly indexed by the ASCII value
  of the target character. This array stores only the array indexes
  (small inters in the range of $[0,47]$) of another pointer array
  that contains up to 48 child pointers. In this way, the storage
  space can be saved comparing with storing pointers directly, because
  each array index only requires one byte. Figure \ref{fig:character
    map} (c) shows a Map 48 structure, where the ASCII values of $a$,
  $b$ and $c$ are 97, 98 and 99 respectively.

\item \textbf{Map 256} (for $49 \leq ndp \leq 256$): The largest map
  type is simply an array of 256 child pointers with each pointer
  initialized to $NULL$. It is used for storing 49 to 256 keys.  In
  this kind of character map, the child node can be found directly
  through the array index which is the ASCII value of the target
  character. Different from other character maps, in Map 256, there is
  only one array, and it is not necessary to carry out the additional
  indirect access. Therefore, if most entries are not empty, this
  representation is also very space efficient. Figure
  \ref{fig:character map} (d) shows a Map 256 structure, where only
  the pointer array is adopted.
\end{itemize}

The pseudocode of searching in a node, whose type is character map, is
given in Algorithm \ref{alg:character map}.

\begin{algorithm}
  \caption{Searching in a node whose type is character map}\scriptsize
  \label{alg:character map}
  \begin{algorithmic}[1]
    \REQUIRE ~~\\
    A tree $node$ whose type is character map\\
    The target character $t\_ch$.
    \ENSURE ~~\\
    The child node of $node.t\_ch$ (possibly $NULL$).
    \STATE
    \STATE $count \leftarrow$ Number of keys in the character map
    \STATE
    \SWITCH{The type of the character map}
    \CASE{\textsf{Map 4}}
    \FOR{$i \leftarrow 0$ to $count-1$}
    \IF{$keys[i]=t\_ch$}
    \RETURN $child\_pointers[i]$
    \ENDIF
    \ENDFOR
    \STATE Report a mismatch.
    \ENDCASE
    \STATE
    \CASE{\textsf{Map 16}}
    \STATE $low \leftarrow 0, high \leftarrow count-1$
    \WHILE{$low \le high$}
    \STATE $mid \leftarrow \lfloor (low+high)/2 \rfloor$
    \IF{$t\_ch=keys[mid]$}
    \RETURN $child\_pointers[mid]$
    \ELSIF{$t\_ch<keys[mid]$}
    \STATE $high \leftarrow mid-1$
    \ELSE
    \STATE $low \leftarrow mid+1$
    \ENDIF
    \ENDWHILE
    \STATE Report a mismatch.
    \ENDCASE
    \STATE
    \CASE{\textsf{Map 48}}
    \IF{$index[t\_ch] \neq NULL$}
    \RETURN $child\_pointers[index[t\_ch]]$
    \ELSE
    \STATE Report a mismatch.
    \ENDIF
    \ENDCASE
    \STATE
    \CASE{\textsf{Map 256}}
    \RETURN $child\_pointers[t\_ch]$
    \ENDCASE
    \ENDPWITCH
  \end{algorithmic}
\end{algorithm}

\subsubsection{String array}
\label{sec:string array}

For the prefix set whose length of keys is greater than 1 ($lss > 1$)
and the number of keys is not greater than 100 ($ndp \leq 100$), the
\emph{string array} structure is adopted as a tree node. Similar to
the Map 4 and Map 16 structures, the keys are stored in
lexicographical order in a separate key array of $ndp \times lss$
bytes, in which each key takes $lss$ bytes. The child pointers are
stored at the corresponding positions in another pointer array. Figure
\ref{fig:string array} illustrates a string array structure holding
three keys: $aaa$, $bbb$ and $ccc$.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.45\textwidth]{./eps/string_array}
  \caption{The string array structure with three keys: $aaa,\, bbb,\,
    ccc$.}
  \label{fig:string array}
\end{figure}

For efficiency, if the number of keys is less than 5, a naive linear
search is used to search the target string in the key array;
otherwise, a more efficient (but complicated) binary search algorithm
is adopted.  The pseudocode of the searching in a node whose type is
string array is depicted in Algorithm \ref{alg:string array}, where we
use the notation $A \prec B$ to denote that string $A$ is
\emph{lexicographically smaller} than string $B$. For the string array
whose number of keys is less than 5, the target string is compared
sequentially with the keys in the key array. If the target string
matched some key, return the child pointer at the corresponding
position in the pointer array; otherwise, report a mismatch. On the
other hand, if the number of keys is greater than 4, the target string
is searched by a binary search routine from the middle element of the
key array.

\begin{algorithm}
  \caption{Searching in a node whose type is string array}\scriptsize
  \label{alg:string array}
  \begin{algorithmic}[1]
    \REQUIRE ~~\\
    A tree $node$ whose type is string array; \\
    The target string $t\_str$.\\
    \ENSURE ~~\\
    The child node of $node.t\_str$ (possibly $NULL$).\\
    \STATE
    \STATE $count \leftarrow$ The number of keys in $node$
    \STATE
    \IF{$count < 5$}
    \STATE $i \leftarrow 0$
    \WHILE{$i < count$ and $keys[i] \prec t\_str$}
    \STATE $i \leftarrow i+1$
    \ENDWHILE
    \IF{$i < count$ and $keys[i]=t\_str$}
    \RETURN $child\_pointers[i]$
    \ELSE
    \STATE Report a mismatch
    \ENDIF
    \ELSE
    \STATE $low \leftarrow 0$, $high \leftarrow count-1$
    \WHILE{$low \leq high$}
    \STATE $mid \leftarrow \lfloor (low+high)/2 \rfloor$
    \IF{$t\_str = keys[mid]$}
    \RETURN $child\_pointers[mid]$
    \ELSIF{$t\_str < keys[mid]$}
    \STATE $high \leftarrow mid - 1$
    \ELSE
    \STATE $low \leftarrow mid + 1$
    \ENDIF
    \ENDWHILE
    \STATE Report a mismatch.
    \ENDIF
  \end{algorithmic}
\end{algorithm}

\subsubsection{Hash table}
\label{sec:hash table}

For the prefix set whose number of keys is greater than 100 ($lss > 1$
and $ndp > 100$), searching in the string array structure becomes
inefficient even using a binary search. In this case, a more fast (but
heavy) data structure --- \emph{hash table} is adopted to deal with
large number of keys. It is worth noting that, the construction of the
hash table is directly based on the suffix set from which the prefix
set is formed rather than the prefix set which is the basis of
building other node structures. In our implementation, a hash table is
an array of child pointers with each pointer initialized to $NULL$,
and we also utilize a \emph{string hash function} which transforms a
string to a positive integer.

% Indeed, when computing $ndp$, we
% merely count rather than really constructing the prefix set and then
% computing its $ndp$

In particular, given a suffix set $SF$, the size of the hash table
(denoted by $table\_size$) is determined by $ndp$ and a given load
factor $lf$ (ratio of $ndp$ to $table\_size$), i.e.
$table\_size = \lceil ndp\,/\,lf \rceil$. For example, with the $ndp$
of 1000 and a load factor of $70\%$, the $table\_size$ is
$\lceil 1000/0.7 \rceil = 1429$. Note that, the $ndp$ here is defined
to be the number of distinct length-$lss$ prefixes of the suffixes in
$SF$, which is equal to the size of the prefix set derived from $SF$
(after removing the repetitions).

Algorithm \ref{alg:hash} shows the pseudocode of building a hash table
based on $SF$. $SF$ is firstly partitioned into small suffix sets by
hashing: for each suffix $suf \in SF$, its prefix $suf^{+lss}$ is
hashed to an integer $i$ between 0 and $table\_size-1$ by the string
hash function, then $suf$ is associated with the $i$-th slot of the
hash table. After that, the suffixes whose length-$lss$ prefixes
hashing to the same value are associated with the same slot of the
hash table. Then, to address the hash collisions, for each slot that
is not $NULL$, the associated suffixes form a small suffix set $SSF$,
and a new tree node is created adaptively based on the prefix set
derived from $SSF$, as we have shown in the \textbf{while} loop of
Algorithm \ref{alg:amt}. The newly created tree node is then
associated with the corresponding slot of the hash table. As a result,
many tree nodes with various types can be associated within one hash
table.

% The suffixes whose length-$lss$ prefixes hashed to the same
% value $i$ are collected together associated with the $i$-th slot and
% forms a small suffix set. (Note that, we do not remove the prefixes of
% the suffixes when partitioning $SF$.) Then, to address the collisions,
% for each non-$NULL$ slot of the hash table, a tree node is built based
% on the small suffix set associated with that slot as stated
% before. Hence, many tree nodes with various structures can be
% incorporated in one hash table.


\begin{algorithm}
  \caption{Building a hash table}\scriptsize
  \label{alg:hash}
  \begin{algorithmic}[1]
    \REQUIRE ~~\\
    The suffix set $SF$ and the load factor $lf$. \\
    \ENSURE ~~\\
    The hash table.\\
    \STATE
    \STATE $ndp \leftarrow$ The number of distinct length-$lss$
    prefixes of suffixes in $SF$
    \STATE $table\_size \leftarrow \lceil ndp\,/\,lf \rceil$
    \STATE $hash\_table \leftarrow$ Create an array  of
    $table\_size$ pointers with each pointer initialized to $NULL$
    \STATE
    \FOR{each $suf \in SF$}
    \STATE $i \leftarrow Hash(suf^{+lss})$
    \STATE Associate $suf$ with $hash\_table[i]$
    \ENDFOR
    \STATE
    \FOR{$i \leftarrow 0$ to $table\_size - 1$}
    \IF{$hash\_table[i]\, \neq\, NULL$}
    \STATE $SSF \leftarrow \{suf\,|\,suf\in SF\; and\; Hash(suf^{+lss})=i\}$
    \STATE $new\_node \leftarrow$ Adaptively create a tree node from
    $SSF$, as shown in the \textbf{while} loop of Algorithm \ref{alg:amt}
    \STATE Associate $new\_node$ with $hash\_table[i]$
    \ENDIF
    \ENDFOR
    \STATE
    \RETURN $hash\_table$
  \end{algorithmic}
\end{algorithm}

Given a target string of length $lss$, the hash value of that string
is computed and used as the index of the hash table. If the
corresponding slot is $NULL$, which means there is a mismatch, the
verification terminates and the engine moves to the next position;
otherwise, the process goes to the tree node associated with that slot
and compares the node with the corresponding substring in the text.

The string hash function adopted can have a significant effect on the
performance of verifying. In our implementation, for each matching
task, the hash function is selected randomly from a \emph{hash
  function family} $H$. The hash function family adopted in our
implementation is the fast \emph{shift-add-xor} hash family proposed
by Ramakrishna \cite{Ramakrishna1997}, which is claimed to be both
uniform and universal. The framework of the hash function family is
shown in Algorithm \ref{alg:hash fun}.

%  satisfied with the
% \textsf{uniformity}, \textsf{universality}, \textsf{applicability} and
% the \textsf{efficiency} properties

%  which should satisfy the following properties:

% \begin{itemize}
% \item \textbf{Uniformity.} If a hash function is uniform then the
% probability of an arbitrary key hashing to a given slot is
% $1/table\_size$, independent of the hash values of other keys. In
% practical terms, uniformity means that for a given load factor average
% access time is roughly constant, regardless of table size.

% \item {Universality.} A hash function family $H$ is universal
% if, for a given $table\_size$ and any pair of valid keys $k_1$ and
% $k_2$, the number of hash functions $h \in H$ such that $h(k_1) =
% h(k_2)$ is less than or equal to $|H|/table\_size$. That is, for a
% randomly-chosen hash function the probability that $k_1$ and $k_2$
% hash to the same value is less than or equal to $1/table\_size$.

% In practice universality means that, with high probability, a
% randomly-chosen hash function will perform well. For any hash function
% it is true that there exist sets of keys that all hash to the same
% value and no hash function is invulnerable to a deliberate attempt to
% identify such a set of keys. However, if a hash function family is
% universal and the functions in the family are uniform then it is
% guaranteed that the family cannot be subjected to such attack.

% \item {Applicability.} At a more pragmatic level, hash
% functions should be applicable in all circumstances where hashing
% might be used. A function that is limited to a few table sizes, can
% only hash strings of a certain length, is not as valuable as functions
% without such restrictions.

% \item {Efficiency}. The primary advantage of hashing as an
% access method is its speed: given $ndp$ keys and a table of $O(ndp)$
% size, the search time is $O(1)$, assuming a hash function with time
% complexity $O(1)$. Hash functions should also be small; in many
% applications there is little advantage to a function that is as large
% as the key set.


\begin{algorithm}
  \caption{The \textsf{shift-add-xor} hash family}\scriptsize
  \label{alg:hash fun}
  \begin{algorithmic}[1]
    \REQUIRE ~~\\
    The string $c_1c_2 \dots c_m$ and the $table\_size$.  \\
    \ENSURE ~~\\
    The hash value of $c_1c_2 \dots c_m$.
    \STATE
    \STATE $value \leftarrow seed$
    \FOR{$i \leftarrow 1$ to $m$}
    \STATE $value \leftarrow value \oplus (value << L + value >> R + c_i)$
    \ENDFOR
    \RETURN $value$ mod $table\_size$
  \end{algorithmic}
\end{algorithm}

Firstly, a random positive integer $seed$ is assigned to the initial
$value$ of the hash function. (Note that, different seeds give
different hash functions, consequently, a hash function family can be
defined by given different seeds.) Then the hash value is computed
gradually from each character of the input string, by using the
combinations of the \textsf{add, shift, xor} operators. In particular,
$\oplus$ is the \textsf{xor} operator, $value << \;L$ ($value >> \;R$)
means shift the $value$ to the left (right) $L$ ($R$) bits and $c_i$
is the ASCII value of the $i$-th character of the input
string. Through testing, we set $L=5$ and $R=2$ in our experiment,
which is able to provide greater likelihood of a uniform distribution
of hash values.

\subsection{Further Improvements}
\label{sec:further improments}

The verification module presented above is pretty efficient, but there
are still some techniques that can further improve the time and space
efficiency of this module.

\subsubsection{Dividing the AMT}
\label{sec:divide amt}

The verification module we have just built is based on a big single
AMT which is constructed from the whole pattern set. Actually, we can
divide the big AMT into small ones and enable each verifying procedure
to refer to only one small AMT, by which the time for verifying can be
reduced. The division is based on the following observations.

As stated in the filter module, for a position $i$ of the text $T$, if
some pattern $p_j$ starts at $i$, there must be
$Hash[T[i+lss-k,k]] = Hash[p_j[lss-k+1,k]]$ (we call
$Hash[p_j[lss-k+1,k]]$ the \emph{signature} of $p_j$). This means that
we can group the patterns according to their signatures: patterns with
the same signature are grouped together to form a sub-pattern-set, and
for each sub-pattern-set we build a (small) AMT from it. In addition,
an index table is adopted to allow the corresponding AMT be retrieved
in $O(1)$ time by its signature. Now, the verification module is made
up of an index table and several small AMTs.

Particularly, once a suspicious position $i$ in $T$ is detected, the
value $Hash[T[i+lss-k,k]]$ is used as the index of the index table to
find the corresponding small AMT for verifying $i$. As the scale of
the AMT shr, the time for each verifying is reduced accordingly.

\section{Experimental Results}
\label{sec:experiments}

In this section, we evaluate the performance of \textsf{AAMPM}, and
compare it with other four algorithms: the \textsf{AC} and \textsf{WM}
algorithms, which are two classic ``baseline'' algorithms for MPM; the
\textsf{MASM} and \textsf{Pre-filter+AC} algorithms, which are two
state-of-the art MPM algorithms with good efficiency in practically
matching. All these algorithms are evaluated in two aspects: the
robustness of these algorithms under pattern sets with various $lsp$s
and the scalability of the algorithms under pattern sets with various
number of patterns.

\subsection{Simulation Settings}

The experiments are conducted on a PC with an Intel core i7 2.93GHz
CPU, 8GB of RAM and 1TB Disk Driver; the operating system is Windows 7
Professional (64-bits). All the testing algorithms are implemented in
C/C++ within the IDE of \emph{Code::Blocks}. The test data used in the
experiments is the real-world English text from the Pizza\;\&\;Chili
corpus. The patterns are extracted randomly from the text to form the
pattern sets.

Next, the parameter configuration in \textsf{AAMPM} in our experiment
are given. For the String Arrays, if the number of strings exceeds
\textbf{4}, the binary search is used to search the target string in
the key array; otherwise, the linear search is adopted. For the Hash
Tables, the load factor is set to \textbf{0.5}; the \emph{seed} of the
hash function is generated randomly in the range of
$\mathbf{1} \sim \mathbf{50}$; the shift values \emph{L} and \emph{R}
are set to \textbf{2} and \textbf{6} respectively. Once the $ndp$ is
lager than \textbf{100}, the Hash Table is built instead of the String
Array. These choices of parameters have been extensively tested on a
wide range of pattern sets, and are considered to be very efficient
for a typical match.

% The data sets include three kinds of data: the real-world English
% text and the DNA sequence from the Pizza\&Chili corpus, the random
% string which is generated artificially.



\subsection{Evaluation under various lsps}

As we've mentioned before, many MPM algorithms are sensitive to the
length of the patterns, particularly, the length of the shortest
pattern ($lsp$) in the pattern set. Therefore, it is necessary to
measure the performance of the \textsf{AAMPM} and compare it with the
other investigated algorithms under pattern sets with various
$lsp$s. In this evaluation, there are totally 9 pattern sets for
testing with $lsp$s ranging from 2 to 10, and every pattern set has a
fixed number of $10^5$ patterns. The size of text string is fixed to
200 MB. The characteristics of the pattern sets are shown in Table
\ref{tab:lsps}, which includes: the length of the shortest pattern
(LSP), the length of the longest pattern (LLP), the average length of
the patterns (ALP), the standard deviation of the pattern lengths
(SD), the total length of the patterns (TLP) and the number of
patterns in the pattern set (Count).

\begin{table}
  \centering
  \caption{The characteristics of pattern sets with various LSPs.}
  \scriptsize
  \label{tab:lsps}
  \begin{tabular}{rrrrrrr}
    \hline
    Pattern Set & LSP  & LLP  & ALP & SD & TLP & Count\\
    \hline
    $P_1$ & 2 & 50 & 25.9 & 14.1 & 2,599,020 & $10^5$\\
    $P_2$ & 3 & 50 & 26.5 & 13.9 & 2,646,762 & $10^5$\\
    $P_3$ & 4 & 50 & 27.0 & 13.6 & 2,704,091 & $10^5$\\
    $P_4$ & 5 & 50 & 27.6 & 13.2 & 2,745,545 & $10^5$\\
    $P_5$ & 6 & 50 & 27.9 & 13.0 & 2,793,178 & $10^5$\\
    $P_6$ & 7 & 50 & 28.4 & 12.7 & 2,843,580 & $10^5$\\
    $P_7$ & 8 & 50 & 29.0 & 12.4 & 2,903,343 & $10^5$\\
    $P_8$ & 9 & 50 & 29.5 & 12.1 & 2,948,372 & $10^5$\\
    $P_9$ &10 & 50 & 30.0 & 11.8 & 2,998,992 & $10^5$\\
    \hline
  \end{tabular}
\end{table}

% For universality, evaluation are performed independently on three
% different kinds of data: the English text, the DNA sequence and the
% random string. These three types of data have completely different
% alphabet and character combination. Therefore this evaluation can
% reflect the robustness of the algorithms for pattern sets with
% various features.

% The performance of the investigated algorithms are measured in
% preprocessing time and the matching time. The former is the time
% consumed by constructing the data structure in the preprocessing
% phase; the latter is used for matching with the text string in the
% matching phase. Usually, most of the algorithms mainly focus on the
% matching time, however, the preprocessing time is also significant
% factor, since a very short preprocessing time means the algorithm
% allows the pattern set to be updated dynamically.


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\textwidth]{./eps/lsp}
  \caption{Matching times for various lsps.}
  \label{fig:lsp}
\end{figure}

Figure \ref{fig:lsp} presents the mean matching time (in seconds) used
by these algorithms based on 5 independent runs.  In general, the
\textsf{AC} algorithm is not affected much by the changes in $lsp$.
However, the DFA built by this algorithm consumes a huge amount of
memory. What's worse is that, the frequent, unpredictable
state/failure transitions will lead to a lot of cache misses during
matching, and these results in the low performance of \textsf{AC}. The
\textsf{WM} algorithm outperforms \textsf{AC} on the pattern sets
whose $lsp$ is larger than 2, even so, for the pattern sets with small
$lsp$, the performance of \textsf{WM} degrades dramatically. This is
due to the ``skip strategy'' of \textsf{WM} algorithm that does not
work well for the pattern sets with small $lsp$s, which means most of
the positions in the text need to be checked by the HASH table of
\textsf{WM}, and this takes a lot of time. The \textsf{Pre-filter+AC}
algorithm outperforms all other algorithms (except \textsf{AAMPM}) for
pattern sets whose $lsp$ is larger than 4. Similarly, this algorithm
also uses a skip strategy: once a position of the text is filtered out
by the Pre-filter, at most the following $lsp-1$ positions can be
skipped over. Consequently, for pattern sets with small $lsp$, this
algorithm can not skip much in the text, which will seriously affect
its performance. In contrast, the \textsf{MASM} algorithm performs
well for small $lsp$ ($lsp \leq 4$). This algorithm first compacts the
pattern set by a prefix tree: if a pattern $A$ is a prefix of another
pattern $B$, $A$ can be merged into $B$. Then the algorithm builds a
binary search tree for the compacted pattern set based on
lexicographical order. Comparing with the long patterns, the short
ones are more likely to be prefixes of others, and thus, more likely
to be eliminated by compacting. Thus, pattern sets with small $lsp$
are well compressed, which will improve both time and space efficiency
of \textsf{MASM} in the matching phase.

From Figure \ref{fig:lsp} we can see that, \textsf{AAMPM} outperforms
all other investigated algorithms on every $lsp$. Comparing with
\textsf{AC}, the matching time is reduced by $85\%$ in average for all
$lsp$s; even comparing with the fast \textsf{Pre-filter+AC} algorithm,
the matching time is reduced by $10\% \sim 20\%$ for medium $lsp$s
($lsp \geq 4$) and by $70\%$ for small $lsp$s ($lsp < 4$). As
mentioned before, this is mainly due to the adaptivity of the nodes of
AMT, which enables the tree nodes as efficient and compact as
possible. Therefore, no matter what the $lsp$s of the pattern sets
are, the corresponding AMT is able to keep high search efficiency as
well as low memory requirement in the matching phase. Owing to this,
the \textsf{AAMPM} has the best robustness for all of the testing
pattern sets. The statistics about node types of corresponding AMTs
are shown in table \ref{tab:node types}. The Single Char and Single
String types in the table are actually the Map 4 and String Array
types which contain only one element, respectively. Obviously, as the
$lsp$ of the pattern set changes, the number of nodes of the each type
in the AMT changes accordingly, which reflects the adaptivity of AMT.

Moreover, the performance of \textsf{AAMPM} is very stable and
reliable. In particular, once the length of the text and number of
patterns are fixed, the matching time changes little for various
$lsp$s. As shown in Figure \ref{fig:lsp}, the matching time of
\textsf{AAMPM} is always about 10 seconds for a text of 200MB and a
pattern set with $10^5$ patterns; for $lsp$s ranging from $2$ to $10$,
the change in the matching time is less than 2 seconds. In fact, as
the $lsp$ increases, the average length of the paths from the root to
the leaves in AMT will increase slightly. Accordingly, in the matching
phase, the matching paths for some text positions may get a little
longer, which might slightly slow down the matching speed.

% Since these volumes are all fixed in this evaluation, the matching
% time is nearly the same under all lsps.

\begin{table}[!htp]
  \centering
  \caption{The statistics about the AMTs under various lsps}
  \scriptsize
  \label{tab:node types}
  \begin{tabular}{rrrrrrrrrr}
 \hline
 Lsp &
 Single Char &
 Map 4 &
 Map 16 &
 Map 48 &
 Map 256 &
 Single String &
 String Array   &
 Hash Table &
 Total\\
 \hline
 2  & 2,464 & 2,096 & 663 & 133 & 0 & 63,274 &  21,627 & 10 & 90,267\\
 3  & 2,379 & 1,636 & 385 & 90  & 0 & 64,701 &  22,551 & 10 & 91,752\\
 4  & 2,273 & 1,329 & 201 & 57  & 0 & 66,162 &  22,736 &  1 & 92,759\\
 5  & 2,045 & 1,104 & 112 & 45  & 0 & 68,152 &  22,048 &  1 & 93,507\\
 6  & 1,758 &   899 &  52 & 34  & 0 & 70,179 &  20,818 &  1 & 93,741\\
 7  & 1,560 &   807 &  29 & 33  & 0 & 72,284 &  19,280 &  1 & 93,994\\
 8  & 1,515 &   803 &  20 & 31  & 0 & 73,447 &  18,274 &  1 & 94,091\\
 9  & 1,441 &   788 &  24 & 30  & 0 & 74,584 &  17,120 &  1 & 93,988\\
10  & 1,361 &   794 &  24 & 29  & 0 & 75,094 &  16,427 &  1 & 93,730\\
\hline
  \end{tabular}
\end{table}

\subsection{Evaluation under various numbers of patterns}

In this section, we evaluate the scalability of the investigated
algorithms by testing them under pattern sets with various numbers of
patterns. As before, the size of the text is fixed to 200MB, and there
are two groups of pattern sets for testing --- the small group and the
large group. The small group contains 9 pattern sets, with the sizes
(number of patterns) increasing from $1 \times 10^5$ to
$9 \times 10^5$ in steps of $10^5$, while the large group contains 10
pattern sets, with the sizes increasing from $10^6$ to $10^7$ in steps
of $10^6$. The range of the pattern length of each pattern set is
fixed to $5 \sim 50$.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\textwidth]{./eps/small_group}
  \caption{Matching times for the small group.}
  \label{fig:small_group}
\end{figure}

For the small group, the mean matching times of the investigated
algorithms under 5 independent runs are shown in Figure
\ref{fig:small_group}. The experimental results indicate that, the
performance of the \textsf{AC} algorithm is relative stable (but not
high) as the number of patterns growing, but it can not deal with
pattern sets whose size is larger than $7 \times 10^5$ in this testing
due to lack of memory. Although the \textsf{WM} algorithm performs
better than \textsf{AC}, it is less stable, for instance, the matching
time only rises about $1s$ as the number of patterns increasing from
$2 \times 10^5$ to $3 \times 10^5$, but when the number of patterns
increases from $8 \times 10^5$ to $9 \times 10^5$, the matching time
rises up to $9s$. Thus, we can not estimate the matching time of
\textsf{WM} based on the size of the pattern set. The
\textsf{Pre-filter+AC} performs well as the number of patterns
growing, but its matching time is also unpredictable, e.g. the
matching times are nearly the same as the number of patterns
increasing from $4 \times 10^5$ to $5 \times 10^5$. The instability of
\textsf{WM} and \textsf{Pre-filter+AC} is mainly due to that: the
effect of the filters used by these two algorithms highly depends on
the contents of the text and patterns themselves, for some pattern
sets whose patterns occurs frequently in the text, the effect of the
filters degrades. On the other hand, as the number of patterns grows,
the matching time of \textsf{MASM} grows more slowly than that of the
\textsf{Pre-filter+AC}, which is due to that the matching time of
\textsf{MASM} mainly relies on the depth of the pipelinded binary
search tree and that depth grows very slow as the number of patterns
increasing.

% For the \textsf{MASM} algorithm, its
% performance is more stable than the \textsf{Pre-filter+AC} algorithm.
% This is because the performance of \textsf{MASM} mainly relies on the
% depth of the pipelined binary search tree, which  as the
% number of patterns increases.

\begin{table}[!htp]
  \scriptsize
  \caption{The statistics about the AMTs for the small group}
  \label{tab:small}
  \begin{tabular}{rrrrrrrrrr}
 \hline
 Size &
 Single Char &
 Map 4 &
 Map 16 &
 Map 48 &
 Map 256 &
 Single String &
 String Array   &
 Hash Table &
 Total\\
\hline
$1 \times 10^5$  &  2,000 &  1,006 &    72 &  18 & 0 &  68,268 &  21,971 & 103 &  93,438 \\
$2 \times 10^5$ &  4,628 &  2,808 &   307 &  42 & 0 & 130,440 &  46,514 & 244 & 184,983 \\
$3 \times 10^5$ &  7,414 &  4,858 &   574 &  84 & 0 & 190,653 &  71,757 & 403 & 275,746 \\
$4 \times 10^5$ & 10,555 &  7,282 &   946 & 139 & 0 & 247,656 &  97,967 & 544 & 365,089 \\
$5 \times 10^5$ & 13,703 & 10,651 & 1,503 & 238 & 4 & 302,350 & 124,528 &  17 & 452,994 \\
$6 \times 10^5$ & 17,335 & 13,536 & 1,954 & 298 & 0 & 356,644 & 151,435 &  31 & 531,233 \\
$7 \times 10^5$ & 21,080 & 16,872 & 2,405 & 388 & 4 & 408,165 & 179,621 &  35 & 628,570 \\
$8 \times 10^5$ & 24,357 & 20,094 & 2,977 & 436 & 3 & 457,674 & 208,426 &  37 & 714,004 \\
$9 \times 10^5$ & 28,356 & 23,924 & 3,441 & 513 & 9 & 504,779 & 238,632 &  42 & 799,696 \\
\hline
  \end{tabular}
\end{table}

Among all these algorithms, the \textsf{AAMPM} is the most efficient
as well as stable one. The high performance of \textsf{AAMPM} is
mainly due to the \emph{root} node of AMT, which is almost always the
Hash Table as the number of patterns grows. As mentioned before, the
root plays the role of a ``filter'', by which a large number of
positions of the text that are impossible to match with any pattern
can be quickly filtered out. Moreover, the increasing in the number of
patterns mainly leads to the growing of the width rather than the
depth of AMT. Therefore, for each position of the text, the checking
time changes slightly. From the experimental results, we can even give
a rough estimate on the matching time based on the size of pattern set
as follows: given the text of 200 MB, an increase of $10^5$ in the
number of patterns will lead to roughly $1$ more second taken in the
matching time. The statistics about the node types of the
corresponding AMTs for the small group are shown in Table
\ref{tab:small}. We can see that an increase of $10^5$ in the number
of patterns will yield about $9 \times 10^4$ tree nodes.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\textwidth]{./eps/large_group}
  \caption{Matching times for the large group.}
  \label{fig:large_group}
\end{figure}

For the large group, the mean matching times of the investigated
algorithms are shown in Figure \ref{fig:large_group}.  From the
results we can see that, as the number of patterns grows from $10^6$
to $10^7$, the increases in the matching time of the investigated
algorithms are: $410\%$ (\textsf{WM}), $374\%$
(\textsf{Pre-filter+AC}), $238\%$ (\textsf{MASM}), $112\%$
(\textsf{AAMPM}) respectively. For every increase of $10^6$ in the
number of patterns, the mean increment in the matching time of the
investigated algorithms are: $23.7s$ (\textsf{WM}), $17.0s$
(\textsf{Pre-filter+AC}), $12.3s$ (\textsf{MASM}), $3.5s$
(\textsf{AAMPM}) respectively. From the results we can see that,
\textsf{Pre-filter+AC} and \textsf{WM} perform not well for large
pattern sets. This is because, as the number of patterns grows, the
chances for every position of the text to successfully match a pattern
increases, which will reduce the effect of the filters of these
algorithms and results in performance degradation. The \textsf{MASM}
algorithm outperforms \textsf{Pre-filter+AC} for pattern sets whose
size are larger than $3 \times 10^6$, which is due to its pattern set
compression strategy and the good scalability of its pipelinded binary
search tree.

\begin{table}[!htp]
  \caption{The statistics about the AMTs for the large group}
  \scriptsize
  \label{tab:large_group}
  \begin{tabular}{rrrrrrrrrr}
 \hline
 Size &
 Single Char &
 Map 4 &
 Map 16 &
 Map 48 &
 Map 256 &
 Single String &
 String Array   &
 Hash Table &
 Total\\
\hline
$1 \times 10^6$ &  38,935 &   27,428  &   6,468 &     851   &	 4 &    588,343	 &    240,886 &  1,027 &    903,942  \\
$2 \times 10^6$ &  85,900 &   68,973  &  17,084 &   2,141   &	20 &  1,091,527	 &    496,001 &  2,015 &  1,763,661  \\
$3 \times 10^6$ & 136,054 &  117,527  &  28,092 &   3,410   &	17 &  1,547,659	 &    764,359 &  2,749 &  2,599,867  \\
$4 \times 10^6$ & 189,926 &  171,155  &  40,328 &   4,716   &	23 &  1,966,326	 &  1,045,558 &  3,450 &  3,421,482  \\
$5 \times 10^6$ & 247,707 &  230,163  &  83,043 &   5,989   &  38 &  2,354,308	 &  1,332,896 &  3,885 &  4,228,029  \\
$6 \times 10^6$ & 304,863 &  293,591  &  65,475 &   7,169   &	49 &  2,706,094	 &  1,625,261 &  4,379 &  5,006,881  \\
$7 \times 10^6$ & 366,861 &  361,695  &  77,899 &   8,434   &	53 &  3,030,673	 &  1,925,027 &  4,733 &  5,775,380  \\
$8 \times 10^6$ & 429,121 &  433,708  &  90,280 &   9,660   &	62 &  3,336,765	 &  2,226,879 &  5,070 &  6,531,545  \\
$9 \times 10^6$ & 494,278 &  509,992  & 102,402 &  10,833   &	72 &  3,623,407	 &  2,537,413 &  5,224 &  7,283,621  \\
$1 \times 10^7$ & 558,241 &  591,455  & 115,012 &  11,922   &	79 &  3,986,683	 &  2,847,277 &  5,505 &  8,026,174  \\
\hline
\end{tabular}
\end{table}

On the other hand, among all the algorithms, \textsf{AAMPM} is the
most efficient algorithm for large pattern sets. An increase of $10^6$
in the number of patterns, only require 3 more seconds to match, this
is mainly due to the good scalability of the AMT which enables the
matching time grows very slow as the number of patterns explodes. The
statistics about the AMTs for the large group are shown in Table
\ref{tab:large_group}. Every $10^6$ increase in the number of patterns
will increase only about $8 \times 10^5$ tree nodes.

\section{Conclusion and Future Works}
\label{sec:conclusion}

In this paper, we propose an Adaptive Architecture for Multi-Pattern
Matching (\textsf{AAMPM}). The \textsf{AAMPM} used a data structure
called Adaptive Matching Tree (AMT) which can be constructed to fit
the feature of the given pattern set. Using the AMT to match with the
text can improve the robustness of \textsf{AAMPM} for pattern sets
with various $lsp$s. Moreover, owing to the scalability and
compactness of AMT, \textsf{AAMPM} offers a good support for
large-scale pattern sets. In the future, we will try to design more
efficient inner structures of the tree nodes; furthermore, since
\textsf{AAMPM} needs to check every position of the text string, we
will further improve this by designing a ``skip scheme'' for
\textsf{AAMPM}.

\section{Acknowledgments}

This work is supported by National Natural Science Foundation of China
(No.61472297).

\bibliography{./DM}


\end{document}



% which means the suffix set is actually the
% original whole pattern set)

% \begin{tikzpicture}[scale=1.8]

% %\filldraw[fill=red, draw=green] (0,0) circle [ radius=1cm];
% \shade[left color=red, right color=black] (0,0) rectangle (1,1);
% \shadedraw[ball color=yellow, draw=balck] (1.5,0.5) circle [radius=0.5];
% \draw (-1.5,0)--(1.5,0);
% \draw (0,-1.5)--(0,1.5);
% \draw[rotate=45] (0,0) rectangle (0.5,0.5);
% \draw (0,0) rectangle (1,1);
% \fill[green!20!white] (0,0) -- (1cm,0) -- arc[start angle=0, end angle=30, radius=1cm]--(0cm,0cm);
% \draw (-1,-1) rectangle (-0.5,-0.5);
% \draw (3mm,0mm) arc [start angle=45, end angle=110, radiu
% s=3mm];$PF$$PF$$PF$
% \end{tikzpicture}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
